{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-18T02:59:08.247012600Z",
     "start_time": "2023-08-18T02:59:08.198010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n基于Louvain算法识别欺诈团伙代码执行顺序：\\n1. 加载Python包；\\n2. 构建全局社交网络；\\n3. 基于Louvain算法对全局网络进行社区划分；\\n4. 基于节点协同分类算法预测非欺诈节点欺诈的概率；\\n5. 找出疑似欺诈团伙；\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "基于Louvain算法识别欺诈团伙代码执行顺序：\n",
    "1. 加载Python包；\n",
    "2. 构建全局社交网络；\n",
    "3. 基于Louvain算法对全局网络进行社区划分；\n",
    "4. 基于节点协同分类算法预测非欺诈节点欺诈的概率；\n",
    "5. 找出疑似欺诈团伙；\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T02:59:12.808282200Z",
     "start_time": "2023-08-18T02:59:12.778278700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 1. 加载所需Python包\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "import community\n",
    "import re\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号'-'显示为方块的问题"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T02:59:13.159903300Z",
     "start_time": "2023-08-18T02:59:13.152900300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全局网络中节点的数量为: 628754\n",
      "全局网络中边的数量为: 635179\n",
      "                                 节点和度数                              节点  度数\n",
      "0  (D0008849988454611aab167219efe6, 2)  D0008849988454611aab167219efe6   2\n",
      "1  (P15608bc52549c2de436006f38713b, 2)  P15608bc52549c2de436006f38713b   2\n",
      "2  (D000d69334e1e4d19a5e3ca29a4e23, 2)  D000d69334e1e4d19a5e3ca29a4e23   2\n",
      "3  (P32b854de8ff19255e57234b12beb0, 2)  P32b854de8ff19255e57234b12beb0   2\n",
      "4  (D001152d44a664c06a34d663321e6b, 2)  D001152d44a664c06a34d663321e6b   2\n",
      "                             node      度中心性\n",
      "0  D0008849988454611aab167219efe6  0.000003\n",
      "1  P15608bc52549c2de436006f38713b  0.000003\n",
      "2  D000d69334e1e4d19a5e3ca29a4e23  0.000003\n",
      "3  P32b854de8ff19255e57234b12beb0  0.000003\n",
      "4  D001152d44a664c06a34d663321e6b  0.000003\n"
     ]
    }
   ],
   "source": [
    "# 2. 构建全局社交网络\n",
    "\n",
    "# path为network_data.csv和fraud_flag_data.csv的存储路径，在实操时相关路径均要换成自己本地的路径\n",
    "path = r'D:\\\\金融风控\\\\Python金融风控策略实践 代码和数据样例\\\\Chapter8\\\\8.3.4基于Louvain算法构建欺诈团伙识别模型\\\\'\n",
    "# 建模结果输出路径\n",
    "path_result = path + 'louvain\\\\'\n",
    "if not os.path.exists(path_result):\n",
    "    os.makedirs(path_result)\n",
    "# 取数，同时为避免数据重复对数据去重\n",
    "f = open(path + 'network_data.csv')\n",
    "mydata = pd.read_csv(f).drop_duplicates()\n",
    "mydata.describe()\n",
    "\n",
    "# 基于networkx包对获取到的数据构建全局网络\n",
    "G = nx.from_pandas_edgelist(mydata, source='source', target='target')\n",
    "\n",
    "'''\n",
    "在构建全局网络时，可为节点设置权重，格式如下：\n",
    "G = nx.from_pandas_edgelist(mydata, source='source',target='target',edge_attr=['weight'])\n",
    "'''\n",
    "\n",
    "print('全局网络中节点的数量为:', G.number_of_nodes())\n",
    "print('全局网络中边的数量为:', G.number_of_edges())\n",
    "\n",
    "# 计算节点的度数\n",
    "degrees = pd.DataFrame(columns=['节点和度数'])\n",
    "degrees['节点和度数'] = pd.Series(G.degree)\n",
    "degrees['节点'] = degrees['节点和度数'].map(lambda x: x[0])\n",
    "degrees['度数'] = degrees['节点和度数'].map(lambda x: x[1])\n",
    "print(degrees.head())\n",
    "\n",
    "# 计算节点的度中心性\n",
    "degree_df = pd.DataFrame(columns=['度中心性'])\n",
    "degree_df['度中心性'] = pd.Series(nx.degree_centrality(G))\n",
    "degree_df = degree_df.reset_index().rename(columns={'index': 'node'})\n",
    "print(degree_df.head())\n",
    "\n",
    "# 计算节点的接近中心性\n",
    "# closeness_df = pd.DataFrame(columns=['接近中心性'])\n",
    "# closeness_df['接近中心性'] = pd.Series(nx.closeness_centrality(G))\n",
    "\n",
    "# 计算节点的中介中心性\n",
    "# betweenness_df = pd.DataFrame(columns=['中介中心性'])\n",
    "# betweenness_df['中介中心性'] = pd.Series(nx.betweenness_centrality(G))\n",
    "# betweenness_df.fillna(0, inplace=True)\n",
    "\n",
    "# 基于网页排名找出活跃节点\n",
    "# pagerank_df = pd.DataFrame(columns=['网页排名'])\n",
    "# pagerank_df['网页排名'] = pd.Series(nx.pagerank(G1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T02:59:19.484364200Z",
     "start_time": "2023-08-18T02:59:16.366365100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始基于Louvain算法进行社区划分\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 基于Louvain算法对全局网络进行社区划分\n",
    "\n",
    "# 社区划分，划分结果为字典形式\n",
    "print('开始基于Louvain算法进行社区划分')\n",
    "part = community.best_partition(G)\n",
    "print('社区划分完成,查看划分结果')\n",
    "print(part)\n",
    "\n",
    "# 将字典转为数据框\n",
    "df = pd.DataFrame.from_dict(part, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "# 数据框包含两列，一列表示节点（node），一列表示节点所属的社区(tag)\n",
    "df.rename(columns={'index': 'node', 0: 'tag'}, inplace=True)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# 节点类型解析函数\n",
    "def match_nodetype(x):\n",
    "    \"\"\" 对节点进行解析，返回每个节点值对应的节点类型\n",
    "    :param x:  要解析的节点\n",
    "    :return:   节点类型\n",
    "    \"\"\"\n",
    "    pattern_u = re.compile('^C')\n",
    "    pattern_p = re.compile('^P')\n",
    "    pattern_d = re.compile('^D')\n",
    "    if pattern_u.match(x):\n",
    "        return '客户节点'\n",
    "    elif pattern_p.match(x):\n",
    "        return '电话节点'\n",
    "    elif pattern_d.match(x):\n",
    "        return '设备节点'\n",
    "\n",
    "\n",
    "# 增加节点类型列，为每个节点匹配上对应的节点类型\n",
    "df['node_type'] = df['node'].map(lambda x: match_nodetype(x))\n",
    "\n",
    "# 关联节点在全局网络中的度中心\n",
    "# df=df.merge(degrees,left_on='node',right_on='node',how='left')\n",
    "# df=df[['tag','node','node_type','node_degree']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:17:32.619217100Z",
     "start_time": "2023-08-18T03:16:53.072223200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 4. 基于节点协同分类算法预测非欺诈节点欺诈的概率\n",
    "\n",
    "# 获取欺诈节点\n",
    "f = open(path + 'fraud_flag_data.csv')\n",
    "fraud_flag_data = pd.read_csv(f).drop_duplicates()\n",
    "\n",
    "# 基于欺诈节点对所有节点进行标注，欺诈为1，否则为0\n",
    "nodes = list(G.nodes())\n",
    "node_df = pd.DataFrame({'nodes': nodes})\n",
    "node_df = pd.merge(node_df, fraud_flag_data, left_on='nodes', right_on='node', how='left')\n",
    "del node_df['node']\n",
    "node_df['fraud_flag'] = node_df['fraud_flag'].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# 首先获取节点初始欺诈概率，欺诈节点欺诈概率为1，未知点欺诈概率为0\n",
    "init_fraud_prob = node_df['fraud_flag'].tolist()\n",
    "fraud_prob = copy.deepcopy(init_fraud_prob)\n",
    "\n",
    "# 设置节点分类算法迭代次数max_iter，基于六度分离理论，迭代次数通常设置为6就行了\n",
    "max_iter = 6\n",
    "# 将每个节点进行编码\n",
    "nodes_i = {nodes[i]: i for i in range(0, len(nodes))}\n",
    "\n",
    "# 欺诈节点\n",
    "fraud_node = node_df['nodes'][node_df['fraud_flag'] == 1].tolist()\n",
    "# 未知节点\n",
    "unlabel_nodes = set(nodes).difference(set(fraud_node))\n",
    "\n",
    "# 开始执行节点协同分类算法，计算未知节点的欺诈概率\n",
    "for i in range(max_iter):\n",
    "    print(i)\n",
    "    pre_fraud_prob = np.copy(fraud_prob)\n",
    "    for unnode in unlabel_nodes:\n",
    "        temp = 0\n",
    "        for item in G.neighbors(unnode):\n",
    "            temp = temp + pre_fraud_prob[nodes_i[item]]\n",
    "        fraud_prob[nodes_i[unnode]] = temp / len(list(G.neighbors(unnode)))\n",
    "\n",
    "# 为每个节点匹配对应的欺诈概率\n",
    "node_df['fraud_prob'] = [round(i, 3) for i in fraud_prob]\n",
    "\n",
    "# 基于每个节点的欺诈概率为节点打上是否欺诈的标签，概率值大于0.5为欺诈，否则为非欺诈\n",
    "node_df['fraud_pred'] = node_df['fraud_prob'].map(lambda x: 1 if x > 0.5 else 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:17:45.674209800Z",
     "start_time": "2023-08-18T03:17:35.161211800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fraud_pred'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'fraud_pred'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphone_flag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnode\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 为预测为欺诈的节点打上标签\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_pred\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_flag\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 计算每个社区对应的疑似欺诈节点占比和节点数\u001B[39;00m\n\u001B[0;32m     13\u001B[0m tag_badrate \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtag\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mreset_index())\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbad_rate\u001B[39m\u001B[38;5;124m'\u001B[39m})\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9423\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[0;32m   9412\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m   9414\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m   9415\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   9416\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   9421\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m   9422\u001B[0m )\n\u001B[1;32m-> 9423\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:678\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[1;32m--> 678\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:798\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    797\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 798\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    800\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:814\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    811\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    812\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m--> 814\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m    816\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m    817\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m    818\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[17], line 10\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      7\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphone_flag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnode\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 为预测为欺诈的节点打上标签\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_pred\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 计算每个社区对应的疑似欺诈节点占比和节点数\u001B[39;00m\n\u001B[0;32m     13\u001B[0m tag_badrate \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtag\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mreset_index())\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_fraud_flag\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbad_rate\u001B[39m\u001B[38;5;124m'\u001B[39m})\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1007\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1004\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m   1006\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m-> 1007\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[0;32m   1010\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[0;32m   1011\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1012\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1116\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1116\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[0;32m   1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[loc]\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3656\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3657\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3658\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3659\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'fraud_pred'"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, node_df[['nodes', 'fraud_flag', 'fraud_prob', 'fraud_pred']], left_on='node', right_on='nodes', how='left')\n",
    "del df['nodes']\n",
    "\n",
    "# 为客户、设备、电话节点打上相应的区分标签\n",
    "df['cust_flag'] = df['node'].map(lambda x: 1 if 'C' in x else 0)\n",
    "df['device_flag'] = df['node'].map(lambda x: 1 if 'D' in x else 0)\n",
    "df['phone_flag'] = df['node'].map(lambda x: 1 if 'P' in x else 0)\n",
    "\n",
    "# 为预测为欺诈的节点打上标签\n",
    "df['pre_fraud_flag'] = df.apply(lambda x: 1 if (x['fraud_pred'] == 1 and x['fraud_flag'] == 0) else 0, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:18:47.800847800Z",
     "start_time": "2023-08-18T03:18:47.031743200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# 计算每个社区对应的疑似欺诈节点占比和节点数\n",
    "tag_badrate = pd.DataFrame(df.groupby(['tag'])['pre_fraud_flag'].mean().reset_index()).rename(columns={'pre_fraud_flag': 'bad_rate'})\n",
    "tag_num = pd.DataFrame(df['tag'].value_counts().reset_index()).rename(columns={'count': 'tag_num'})\n",
    "df_01 = pd.merge(df, tag_badrate, left_on='tag', right_on='tag', how='left')\n",
    "df_02 = pd.merge(df_01, tag_num, left_on='tag', right_on='tag', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:31:32.029175300Z",
     "start_time": "2023-08-18T03:31:31.818175400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tag  bad_rate\n",
      "0            0       0.0\n",
      "1            1       0.0\n",
      "2            2       0.0\n",
      "3            3       0.0\n",
      "4            4       0.0\n",
      "...        ...       ...\n",
      "207373  207373       0.0\n",
      "207374  207374       0.0\n",
      "207375  207375       0.0\n",
      "207376  207376       0.0\n",
      "207377  207377       0.0\n",
      "\n",
      "[207378 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:31:10.647061300Z",
     "start_time": "2023-08-18T03:31:10.614061200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'fraud_flag_x', 'fraud_pred_x', 'fraud_prob_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMergeError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 5. 找出疑似欺诈团伙\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 将社区划分结果与节点欺诈概率预测结果进行关联\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnodes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_flag\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_prob\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfraud_pred\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleft_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnode\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mright_on\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnodes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mleft\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnodes\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 为客户、设备、电话节点打上相应的区分标签\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:162\u001B[0m, in \u001B[0;36mmerge\u001B[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;129m@Substitution\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mleft : DataFrame or named Series\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    132\u001B[0m \u001B[38;5;129m@Appender\u001B[39m(_merge_doc, indents\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmerge\u001B[39m(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    146\u001B[0m     validate: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    147\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m    148\u001B[0m     op \u001B[38;5;241m=\u001B[39m _MergeOperation(\n\u001B[0;32m    149\u001B[0m         left,\n\u001B[0;32m    150\u001B[0m         right,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    160\u001B[0m         validate\u001B[38;5;241m=\u001B[39mvalidate,\n\u001B[0;32m    161\u001B[0m     )\n\u001B[1;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:811\u001B[0m, in \u001B[0;36m_MergeOperation.get_result\u001B[1;34m(self, copy)\u001B[0m\n\u001B[0;32m    807\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indicator_pre_merge(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright)\n\u001B[0;32m    809\u001B[0m join_index, left_indexer, right_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_join_info()\n\u001B[1;32m--> 811\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_and_concat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjoin_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleft_indexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mright_indexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m    813\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    814\u001B[0m result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_type)\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindicator:\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:763\u001B[0m, in \u001B[0;36m_MergeOperation._reindex_and_concat\u001B[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001B[0m\n\u001B[0;32m    760\u001B[0m left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft[:]\n\u001B[0;32m    761\u001B[0m right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright[:]\n\u001B[1;32m--> 763\u001B[0m llabels, rlabels \u001B[38;5;241m=\u001B[39m \u001B[43m_items_overlap_with_suffix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mleft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_info_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mright\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_info_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuffixes\u001B[49m\n\u001B[0;32m    765\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m left_indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_range_indexer(left_indexer, \u001B[38;5;28mlen\u001B[39m(left)):\n\u001B[0;32m    768\u001B[0m     \u001B[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001B[39;00m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001B[39;00m\n\u001B[0;32m    770\u001B[0m     \u001B[38;5;66;03m#  a MultiIndex for left.index.\u001B[39;00m\n\u001B[0;32m    771\u001B[0m     lmgr \u001B[38;5;241m=\u001B[39m left\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mreindex_indexer(\n\u001B[0;32m    772\u001B[0m         join_index,\n\u001B[0;32m    773\u001B[0m         left_indexer,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    778\u001B[0m         use_na_proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    779\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2640\u001B[0m, in \u001B[0;36m_items_overlap_with_suffix\u001B[1;34m(left, right, suffixes)\u001B[0m\n\u001B[0;32m   2638\u001B[0m     dups\u001B[38;5;241m.\u001B[39mextend(rlabels[(rlabels\u001B[38;5;241m.\u001B[39mduplicated()) \u001B[38;5;241m&\u001B[39m (\u001B[38;5;241m~\u001B[39mright\u001B[38;5;241m.\u001B[39mduplicated())]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m   2639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dups:\n\u001B[1;32m-> 2640\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MergeError(\n\u001B[0;32m   2641\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msuffixes\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m which cause duplicate columns \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mset\u001B[39m(dups)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2642\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot allowed.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2643\u001B[0m     )\n\u001B[0;32m   2645\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m llabels, rlabels\n",
      "\u001B[1;31mMergeError\u001B[0m: Passing 'suffixes' which cause duplicate columns {'fraud_flag_x', 'fraud_pred_x', 'fraud_prob_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "# 5. 找出疑似欺诈团伙\n",
    "\n",
    "# 将社区划分结果与节点欺诈概率预测结果进行关联\n",
    "df = pd.merge(df, node_df[['nodes', 'fraud_flag', 'fraud_prob', 'fraud_pred']], left_on='node',right_on='nodes', how='left')\n",
    "del df['nodes']\n",
    "\n",
    "# 为客户、设备、电话节点打上相应的区分标签\n",
    "df['cust_flag'] = df['node'].map(lambda x: 1 if 'C' in x else 0)\n",
    "df['device_flag'] = df['node'].map(lambda x: 1 if 'D' in x else 0)\n",
    "df['phone_flag'] = df['node'].map(lambda x: 1 if 'P' in x else 0)\n",
    "\n",
    "# 为预测为欺诈的节点打上标签\n",
    "df['pre_fraud_flag'] = df.apply(lambda x: 1 if (x['fraud_pred'] == 1 and x['fraud_flag'] == 0) else 0, axis=1)\n",
    "\n",
    "# 计算每个社区对应的疑似欺诈节点占比和节点数\n",
    "tag_badrate = pd.DataFrame(df.groupby(['tag'])['pre_fraud_flag'].mean().reset_index()).rename(columns={'pre_fraud_flag': 'bad_rate'})\n",
    "tag_num = pd.DataFrame(df['tag'].value_counts().reset_index()).rename(columns={'count': 'tag_num'})\n",
    "df_01 = pd.merge(df, tag_badrate, left_on='tag', right_on='tag', how='left')\n",
    "df_02 = pd.merge(df_01, tag_num, left_on='tag', right_on='tag', how='left')\n",
    "\n",
    "# 基于特定的业务逻辑筛选疑似欺诈社区，具体筛选逻辑可结合实际情况灵活调整。本次筛选节点数大于4且疑似欺诈节点占比大于0的社区认为是疑似欺诈团队\n",
    "select_tag = df_02[(df_02['tag_num'] > 4) & (df_02['bad_rate'] > 0)]['tag'].unique()\n",
    "# 打印筛选的社区\n",
    "print(select_tag)\n",
    "\n",
    "# 画出疑似欺诈团伙社交网络图\n",
    "for tag in select_tag:\n",
    "    tag = int(tag)\n",
    "    print('疑似欺诈团伙tag为：', tag)\n",
    "    nodelist = df.loc[df['tag'] == tag, 'node'].tolist()\n",
    "    print('疑似欺诈团伙节点数为:' + str(len(nodelist)))\n",
    "    blacklist = df[(df['tag'] == tag) & (df['fraud_flag'] == 1)]['node'].tolist()\n",
    "    pre_blacklist = df[(df['tag'] == tag) & (df['pre_fraud_flag'] == 1)]['node'].tolist()\n",
    "    # 团伙中欺诈点占比\n",
    "    black_rate = '{:.2%}'.format(len(blacklist) / len(nodelist))\n",
    "    # 团伙中预测为欺诈节点的节点占比\n",
    "    pre_black_rate = '{:.2%}'.format(len(pre_blacklist) / len(nodelist))\n",
    "    edgelist = [i for i in G.edges if i[0] in nodelist and i[1] in nodelist]\n",
    "    # 构建一个无向图\n",
    "    g = nx.Graph()\n",
    "    # 将团伙中的节点添加到无向图网络中\n",
    "    for i in nodelist:\n",
    "        g.add_node(i)\n",
    "\n",
    "    for e in edgelist:\n",
    "        g.add_edge(e[0], e[1])\n",
    "\n",
    "    pos = nx.spring_layout(g)\n",
    "    # 基于欺诈团伙节点重构社交网络，并计算网络中节点的度中心性\n",
    "    subdegree_df = pd.DataFrame(columns=['度中心性'])\n",
    "    subdegree_df['度中心性'] = pd.Series(nx.degree_centrality(g))\n",
    "    # 获取度中心最大的top10节点\n",
    "    top = subdegree_df.sort_values('度中心性', ascending=False)[:10]\n",
    "    top = top.index.values.tolist()\n",
    "    # 获取度中心性最大的top10节点中的非欺诈和非预测欺诈的节点\n",
    "    top = set(top) - set(pre_blacklist) - set(blacklist)\n",
    "    print(top)\n",
    "\n",
    "    # 画图时度中心性越大，节点在图中的形状越大\n",
    "    plt.figure(figsize=(24, 11), dpi=180)\n",
    "    node_color = [g.degree(v) for v in g]\n",
    "    node_size = [5000 * nx.degree_centrality(g)[v] for v in g]\n",
    "\n",
    "    pos = nx.spring_layout(g)\n",
    "    nx.draw_networkx(g, pos, node_size=node_size, node_color=node_color, alpha=0.8, with_labels=False)\n",
    "\n",
    "    # 黑节点在图中的字体颜色为黑色\n",
    "    black_labels = {role: role for role in blacklist}\n",
    "    nx.draw_networkx_labels(g, pos, labels=black_labels, font_color='k', font_size=16)\n",
    "    ## 预测的黑节点在图中的字体颜色为红色\n",
    "    pre_labels = {role: role for role in pre_blacklist}\n",
    "    nx.draw_networkx_labels(g, pos, labels=pre_labels, font_color='r', font_size=15)\n",
    "\n",
    "    # 度中心性最大的top10节点中的非欺诈和非预测欺诈的节点在图中的字体颜色为蓝色\n",
    "    if len(top) > 0:\n",
    "        top_labels = {role: role for role in top}\n",
    "        nx.draw_networkx_labels(g, pos, labels=top_labels, font_color='blue', font_size=14)\n",
    "    plt.title('Tag:' + str(tag) + ',节点数:' + str(\n",
    "        len(nodelist)) + ',黑节点占比为:' + black_rate + ',预测为坏的节点占比为:' + pre_black_rate, fontsize=20)\n",
    "\n",
    "    plt.savefig(path_result + 'tag_' + str(tag) + '.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T03:32:03.236090400Z",
     "start_time": "2023-08-18T03:32:02.985090400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
